{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGEyWodMglbN"
   },
   "source": [
    "### NOTICE: \n",
    "* need to change 'data_dir' for data directory in part2. import data\n",
    "* resouces:   \n",
    "  - https://github.com/domluna/memn2n\n",
    "  - https://devblogs.nvidia.com/optimizing-end-to-end-memory-networks-using-sigopt-gpus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPIlXMSdX8uP"
   },
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLxLeXjvW1Cn"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "def load_task(data_dir, task_id, only_supporting=False):\n",
    "    '''Load the nth task. There are 20 tasks in total.\n",
    "    Returns a tuple containing the training and testing data for the task.\n",
    "    '''\n",
    "    assert task_id > 0 and task_id < 21\n",
    "\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [os.path.join(data_dir, f) for f in files]\n",
    "    s = 'qa{}_'.format(task_id)\n",
    "    train_file = [f for f in files if s in f and 'train' in f][0]\n",
    "    test_file = [f for f in files if s in f and 'test' in f][0]\n",
    "    train_data = get_stories(train_file, only_supporting)\n",
    "    test_data = get_stories(test_file, only_supporting)\n",
    "    return train_data, test_data\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbI tasks format\n",
    "    If only_supporting is true, only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = str.lower(line)\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line: # question\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            #a = tokenize(a)\n",
    "            # answer is one vocab word even if it's actually multiple words\n",
    "            a = [a]\n",
    "            substory = None\n",
    "\n",
    "            # remove question marks\n",
    "            if q[-1] == \"?\":\n",
    "                q = q[:-1]\n",
    "\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else: # regular sentence\n",
    "            # remove periods\n",
    "            sent = tokenize(line)\n",
    "            if sent[-1] == \".\":\n",
    "                sent = sent[:-1]\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False):\n",
    "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    with open(f) as f:\n",
    "        return parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "\n",
    "def vectorize_data(data, word_idx, sentence_size, memory_size):\n",
    "    \"\"\"\n",
    "    Vectorize stories and queries.\n",
    "    If a sentence length < sentence_size, the sentence will be padded with 0's.\n",
    "    If a story length < memory_size, the story will be padded with empty memories.\n",
    "    Empty memories are 1-D arrays of length sentence_size filled with 0's.\n",
    "    The answer array is returned as a one-hot encoding.\n",
    "    \"\"\"\n",
    "    S = []\n",
    "    Q = []\n",
    "    A = []\n",
    "    for story, query, answer in data:\n",
    "        ss = []\n",
    "        for i, sentence in enumerate(story, 1):\n",
    "            ls = max(0, sentence_size - len(sentence))\n",
    "            ss.append([word_idx[w] for w in sentence] + [0] * ls)\n",
    "\n",
    "        # take only the most recent sentences that fit in memory\n",
    "        ss = ss[::-1][:memory_size][::-1]\n",
    "\n",
    "        # Make the last word of each sentence the time 'word' which \n",
    "        # corresponds to vector of lookup table\n",
    "        for i in range(len(ss)):\n",
    "            ss[i][-1] = len(word_idx) - memory_size - i + len(ss)\n",
    "\n",
    "        # pad to memory_size\n",
    "        lm = max(0, memory_size - len(ss))\n",
    "        for _ in range(lm):\n",
    "            ss.append([0] * sentence_size)\n",
    "\n",
    "        lq = max(0, sentence_size - len(query))\n",
    "        q = [word_idx[w] for w in query] + [0] * lq\n",
    "\n",
    "        y = np.zeros(len(word_idx) + 1) # 0 is reserved for nil word\n",
    "        for a in answer:\n",
    "            y[word_idx[a]] = 1\n",
    "\n",
    "        S.append(ss)\n",
    "        Q.append(q)\n",
    "        A.append(y)\n",
    "    return np.array(S), np.array(Q), np.array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2884,
     "status": "ok",
     "timestamp": 1574063435838,
     "user": {
      "displayName": "jinglin zhao",
      "photoUrl": "",
      "userId": "00443336287763192682"
     },
     "user_tz": 360
    },
    "id": "3BTKSYydXMI9",
    "outputId": "24f0559e-3efe-4fda-e657-110a84c5ed06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End-To-End Memory Networks.\\nThe implementation is based on http://arxiv.org/abs/1503.08895 [1]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#MemN2N package\n",
    "\n",
    "\"\"\"End-To-End Memory Networks.\n",
    "The implementation is based on http://arxiv.org/abs/1503.08895 [1]\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "def position_encoding(sentence_size, embedding_size):\n",
    "    \"\"\"\n",
    "    Position Encoding described in section 4.1 [1]\n",
    "    \"\"\"\n",
    "    encoding = np.ones((embedding_size, sentence_size), dtype=np.float32)\n",
    "    ls = sentence_size+1\n",
    "    le = embedding_size+1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            encoding[i-1, j-1] = (i - (embedding_size+1)/2) * (j - (sentence_size+1)/2)\n",
    "    encoding = 1 + 4 * encoding / embedding_size / sentence_size\n",
    "    # Make position encoding of time words identity to avoid modifying them \n",
    "    encoding[:, -1] = 1.0\n",
    "    return np.transpose(encoding)\n",
    "\n",
    "def zero_nil_slot(t, name=None):\n",
    "    \"\"\"\n",
    "    Overwrites the nil_slot (first row) of the input Tensor with zeros.\n",
    "    The nil_slot is a dummy slot and should not be trained and influence\n",
    "    the training algorithm.\n",
    "    \"\"\"\n",
    "    with tf.op_scope([t], name, \"zero_nil_slot\") as name:\n",
    "        t = tf.convert_to_tensor(t, name=\"t\")\n",
    "        s = tf.shape(t)[1]\n",
    "        z = tf.zeros(tf.stack([1, s]))\n",
    "        return tf.concat(axis=0, values=[z, tf.slice(t, [1, 0], [-1, -1])], name=name)\n",
    "\n",
    "def add_gradient_noise(t, stddev=1e-3, name=None):\n",
    "    \"\"\"\n",
    "    Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n",
    "    The input Tensor `t` should be a gradient.\n",
    "    The output will be `t` + gaussian noise.\n",
    "    0.001 was said to be a good fixed value for memory networks [2].\n",
    "    \"\"\"\n",
    "    with tf.op_scope([t, stddev], name, \"add_gradient_noise\") as name:\n",
    "        t = tf.convert_to_tensor(t, name=\"t\")\n",
    "        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n",
    "        return tf.add(t, gn, name=name)\n",
    "\n",
    "class MemN2N(object):\n",
    "    \"\"\"End-To-End Memory Network.\"\"\"\n",
    "    def __init__(self, batch_size, vocab_size, sentence_size, memory_size, embedding_size,\n",
    "        hops=3,\n",
    "        max_grad_norm=40.0,\n",
    "        nonlin=None,\n",
    "        initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "        encoding=position_encoding,\n",
    "        session=tf.compat.v1.Session(),\n",
    "        name='MemN2N'):\n",
    "        \"\"\"Creates an End-To-End Memory Network\n",
    "        Args:\n",
    "            batch_size: The size of the batch.\n",
    "            vocab_size: The size of the vocabulary (should include the nil word). The nil word\n",
    "            one-hot encoding should be 0.\n",
    "            sentence_size: The max size of a sentence in the data. All sentences should be padded\n",
    "            to this length. If padding is required it should be done with nil one-hot encoding (0).\n",
    "            memory_size: The max size of the memory. Since Tensorflow currently does not support jagged arrays\n",
    "            all memories must be padded to this length. If padding is required, the extra memories should be\n",
    "            empty memories; memories filled with the nil word ([0, 0, 0, ......, 0]).\n",
    "            embedding_size: The size of the word embedding.\n",
    "            hops: The number of hops. A hop consists of reading and addressing a memory slot.\n",
    "            Defaults to `3`.\n",
    "            max_grad_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n",
    "            nonlin: Non-linearity. Defaults to `None`.\n",
    "            initializer: Weight initializer. Defaults to `tf.random_normal_initializer(stddev=0.1)`.\n",
    "            optimizer: Optimizer algorithm used for SGD. Defaults to `tf.train.AdamOptimizer(learning_rate=1e-2)`.\n",
    "            encoding: A function returning a 2D Tensor (sentence_size, embedding_size). Defaults to `position_encoding`.\n",
    "            session: Tensorflow Session the model is run with. Defaults to `tf.Session()`.\n",
    "            name: Name of the End-To-End Memory Network. Defaults to `MemN2N`.\n",
    "        \"\"\"\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._vocab_size = vocab_size\n",
    "        self._sentence_size = sentence_size\n",
    "        self._memory_size = memory_size\n",
    "        self._embedding_size = embedding_size\n",
    "        self._hops = hops\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._nonlin = nonlin\n",
    "        self._init = initializer\n",
    "        self._name = name\n",
    "\n",
    "        self._build_inputs()\n",
    "        self._build_vars()\n",
    "\n",
    "        self._opt = tf.train.GradientDescentOptimizer(learning_rate=self._lr)\n",
    "\n",
    "        self._encoding = tf.constant(encoding(self._sentence_size, self._embedding_size), name=\"encoding\")\n",
    "\n",
    "        # cross entropy\n",
    "        logits = self._inference(self._stories, self._queries) # (batch_size, vocab_size)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(self._answers, tf.float32), name=\"cross_entropy\")\n",
    "        cross_entropy_sum = tf.reduce_sum(cross_entropy, name=\"cross_entropy_sum\")\n",
    "\n",
    "        # loss op\n",
    "        loss_op = cross_entropy_sum\n",
    "\n",
    "        # gradient pipeline\n",
    "        grads_and_vars = self._opt.compute_gradients(loss_op)\n",
    "        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) for g,v in grads_and_vars]\n",
    "        grads_and_vars = [(add_gradient_noise(g), v) for g,v in grads_and_vars]\n",
    "        nil_grads_and_vars = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if v.name in self._nil_vars:\n",
    "                nil_grads_and_vars.append((zero_nil_slot(g), v))\n",
    "            else:\n",
    "                nil_grads_and_vars.append((g, v))\n",
    "        train_op = self._opt.apply_gradients(nil_grads_and_vars, name=\"train_op\")\n",
    "\n",
    "        # predict ops\n",
    "        predict_op = tf.argmax(logits, 1, name=\"predict_op\")\n",
    "        predict_proba_op = tf.nn.softmax(logits, name=\"predict_proba_op\")\n",
    "        predict_log_proba_op = tf.log(predict_proba_op, name=\"predict_log_proba_op\")\n",
    "\n",
    "        # assign ops\n",
    "        self.loss_op = loss_op\n",
    "        self.predict_op = predict_op\n",
    "        self.predict_proba_op = predict_proba_op\n",
    "        self.predict_log_proba_op = predict_log_proba_op\n",
    "        self.train_op = train_op\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self._sess = session\n",
    "        self._sess.run(init_op)\n",
    "\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        self._stories = tf.placeholder(tf.int32, [None, self._memory_size, self._sentence_size], name=\"stories\")\n",
    "        self._queries = tf.placeholder(tf.int32, [None, self._sentence_size], name=\"queries\")\n",
    "        self._answers = tf.placeholder(tf.int32, [None, self._vocab_size], name=\"answers\")\n",
    "        self._lr = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n",
    "\n",
    "    def _build_vars(self):\n",
    "        with tf.variable_scope(self._name):\n",
    "            nil_word_slot = tf.zeros([1, self._embedding_size])\n",
    "            A = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "            C = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "\n",
    "            self.A_1 = tf.Variable(A, name=\"A\")\n",
    "\n",
    "            self.C = []\n",
    "\n",
    "            for hopn in range(self._hops):\n",
    "                with tf.variable_scope('hop_{}'.format(hopn)):\n",
    "                    self.C.append(tf.Variable(C, name=\"C\"))\n",
    "\n",
    "            # Dont use projection for layerwise weight sharing\n",
    "            # self.H = tf.Variable(self._init([self._embedding_size, self._embedding_size]), name=\"H\")\n",
    "\n",
    "            # Use final C as replacement for W\n",
    "            # self.W = tf.Variable(self._init([self._embedding_size, self._vocab_size]), name=\"W\")\n",
    "\n",
    "        self._nil_vars = set([self.A_1.name] + [x.name for x in self.C])\n",
    "\n",
    "    def _inference(self, stories, queries):\n",
    "        with tf.variable_scope(self._name):\n",
    "            # Use A_1 for thee question embedding as per Adjacent Weight Sharing\n",
    "            q_emb = tf.nn.embedding_lookup(self.A_1, queries)\n",
    "            u_0 = tf.reduce_sum(q_emb * self._encoding, 1)\n",
    "            u = [u_0]\n",
    "\n",
    "            for hopn in range(self._hops):\n",
    "                if hopn == 0:\n",
    "                    m_emb_A = tf.nn.embedding_lookup(self.A_1, stories)\n",
    "                    m_A = tf.reduce_sum(m_emb_A * self._encoding, 2)\n",
    "\n",
    "                else:\n",
    "                    with tf.variable_scope('hop_{}'.format(hopn - 1)):\n",
    "                        m_emb_A = tf.nn.embedding_lookup(self.C[hopn - 1], stories)\n",
    "                        m_A = tf.reduce_sum(m_emb_A * self._encoding, 2)\n",
    "\n",
    "                # hack to get around no reduce_dot\n",
    "                u_temp = tf.transpose(tf.expand_dims(u[-1], -1), [0, 2, 1])\n",
    "                dotted = tf.reduce_sum(m_A * u_temp, 2)\n",
    "\n",
    "                # Calculate probabilities\n",
    "                probs = tf.nn.softmax(dotted)\n",
    "\n",
    "                probs_temp = tf.transpose(tf.expand_dims(probs, -1), [0, 2, 1])\n",
    "                with tf.variable_scope('hop_{}'.format(hopn)):\n",
    "                    m_emb_C = tf.nn.embedding_lookup(self.C[hopn], stories)\n",
    "                m_C = tf.reduce_sum(m_emb_C * self._encoding, 2)\n",
    "\n",
    "                c_temp = tf.transpose(m_C, [0, 2, 1])\n",
    "                o_k = tf.reduce_sum(c_temp * probs_temp, 2)\n",
    "\n",
    "                # Dont use projection layer for adj weight sharing\n",
    "                # u_k = tf.matmul(u[-1], self.H) + o_k\n",
    "\n",
    "                u_k = u[-1] + o_k\n",
    "\n",
    "                # nonlinearity\n",
    "                if self._nonlin:\n",
    "                    u_k = nonlin(u_k)\n",
    "\n",
    "                u.append(u_k)\n",
    "\n",
    "            # Use last C for output (transposed)\n",
    "            with tf.variable_scope('hop_{}'.format(self._hops)):\n",
    "                return tf.matmul(u_k, tf.transpose(self.C[-1], [1,0]))\n",
    "\n",
    "    def batch_fit(self, stories, queries, answers, learning_rate):\n",
    "        \"\"\"Runs the training algorithm over the passed batch\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "            answers: Tensor (None, vocab_size)\n",
    "        Returns:\n",
    "            loss: floating-point number, the loss computed for the batch\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries, self._answers: answers, self._lr: learning_rate}\n",
    "        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, stories, queries):\n",
    "        \"\"\"Predicts answers as one-hot encoding.\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "        Returns:\n",
    "            answers: Tensor (None, vocab_size)\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries}\n",
    "        return self._sess.run(self.predict_op, feed_dict=feed_dict)\n",
    "\n",
    "    def predict_proba(self, stories, queries):\n",
    "        \"\"\"Predicts probabilities of answers.\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "        Returns:\n",
    "            answers: Tensor (None, vocab_size)\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries}\n",
    "        return self._sess.run(self.predict_proba_op, feed_dict=feed_dict)\n",
    "\n",
    "    def predict_log_proba(self, stories, queries):\n",
    "        \"\"\"Predicts log probabilities of answers.\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "        Returns:\n",
    "            answers: Tensor (None, vocab_size)\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries}\n",
    "        return self._sess.run(self.predict_log_proba_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2873,
     "status": "ok",
     "timestamp": 1574063435840,
     "user": {
      "displayName": "jinglin zhao",
      "photoUrl": "",
      "userId": "00443336287763192682"
     },
     "user_tz": 360
    },
    "id": "YNq5VH47Xdee",
    "outputId": "2c97ebdf-a38a-4283-f80f-dd6465edfba5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Utilities for downloading data from WMT, tokenizing, vocabularies.'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_utils package\n",
    "\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Utilities for downloading data from WMT, tokenizing, vocabularies.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(br\"\\d\")\n",
    "\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "  \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "  words = []\n",
    "  for space_separated_fragment in sentence.strip().split():\n",
    "    words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n",
    "  return [w for w in words if w]\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "    \n",
    "  if not gfile.Exists(vocabulary_path):\n",
    "    print(\"Creating vocabulary %s from %s\" % (vocabulary_path, data_path))\n",
    "    vocab = {}\n",
    "    with gfile.GFile(data_path, mode=\"rb\") as f:\n",
    "      counter = 0\n",
    "      for line in f:\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  processing line %d\" % counter)\n",
    "        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "        for w in tokens:\n",
    "          word = re.sub(_DIGIT_RE, b\"0\", w) if normalize_digits else w\n",
    "          if word in vocab:\n",
    "            vocab[word] += 1\n",
    "          else:\n",
    "            vocab[word] = 1\n",
    "      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "      print('>> Full Vocabulary Size :',len(vocab_list))\n",
    "      if len(vocab_list) > max_vocabulary_size:\n",
    "        vocab_list = vocab_list[:max_vocabulary_size]\n",
    "      with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
    "        for w in vocab_list:\n",
    "          vocab_file.write(w + b\"\\n\")\n",
    "\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "\n",
    "  if gfile.Exists(vocabulary_path):\n",
    "    rev_vocab = []\n",
    "    with gfile.GFile(vocabulary_path, mode=\"rb\") as f:\n",
    "      rev_vocab.extend(f.readlines())\n",
    "    rev_vocab = [line.strip() for line in rev_vocab]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    return vocab, rev_vocab\n",
    "  else:\n",
    "    raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary, tokenizer=None, normalize_digits=True):\n",
    "\n",
    "  if tokenizer:\n",
    "    words = tokenizer(sentence)\n",
    "  else:\n",
    "    words = basic_tokenizer(sentence)\n",
    "  if not normalize_digits:\n",
    "    return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "  # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "  return [vocabulary.get(re.sub(_DIGIT_RE, b\"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "\n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "\n",
    "  if not gfile.Exists(target_path):\n",
    "    print(\"Tokenizing data in %s\" % data_path)\n",
    "    vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "    with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
    "      with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "        counter = 0\n",
    "        for line in data_file:\n",
    "          counter += 1\n",
    "          if counter % 100000 == 0:\n",
    "            print(\"  tokenizing line %d\" % counter)\n",
    "          token_ids = sentence_to_token_ids(line, vocab, tokenizer,\n",
    "                                            normalize_digits)\n",
    "          tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def prepare_custom_data(working_directory, train_enc, train_dec, test_enc, test_dec, enc_vocabulary_size, dec_vocabulary_size, tokenizer=None):\n",
    "\n",
    "    # Create vocabularies of the appropriate sizes.\n",
    "    enc_vocab_path = os.path.join(working_directory, \"vocab%d.enc\" % enc_vocabulary_size)\n",
    "    dec_vocab_path = os.path.join(working_directory, \"vocab%d.dec\" % dec_vocabulary_size)\n",
    "    create_vocabulary(enc_vocab_path, train_enc, enc_vocabulary_size, tokenizer)\n",
    "    create_vocabulary(dec_vocab_path, train_dec, dec_vocabulary_size, tokenizer)\n",
    "\n",
    "    # Create token ids for the training data.\n",
    "    enc_train_ids_path = train_enc + (\".ids%d\" % enc_vocabulary_size)\n",
    "    dec_train_ids_path = train_dec + (\".ids%d\" % dec_vocabulary_size)\n",
    "    data_to_token_ids(train_enc, enc_train_ids_path, enc_vocab_path, tokenizer)\n",
    "    data_to_token_ids(train_dec, dec_train_ids_path, dec_vocab_path, tokenizer)\n",
    "\n",
    "    # Create token ids for the development data.\n",
    "    enc_dev_ids_path = test_enc + (\".ids%d\" % enc_vocabulary_size)\n",
    "    dec_dev_ids_path = test_dec + (\".ids%d\" % dec_vocabulary_size)\n",
    "    data_to_token_ids(test_enc, enc_dev_ids_path, enc_vocab_path, tokenizer)\n",
    "    data_to_token_ids(test_dec, dec_dev_ids_path, dec_vocab_path, tokenizer)\n",
    "\n",
    "    return (enc_train_ids_path, dec_train_ids_path, enc_dev_ids_path, dec_dev_ids_path, enc_vocab_path, dec_vocab_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ij1yGrv-YNEZ"
   },
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3992,
     "status": "ok",
     "timestamp": 1574063436968,
     "user": {
      "displayName": "jinglin zhao",
      "photoUrl": "",
      "userId": "00443336287763192682"
     },
     "user_tz": 360
    },
    "id": "1eZUct5_VTVW",
    "outputId": "d799c973-5c40-4af1-ad29-10dcb2d01e8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example running MemN2N on a single bAbI task.\\nDownload tasks from facebook.ai/babi '"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using multiple tasks\n",
    "\n",
    "\"\"\"Example running MemN2N on a single bAbI task.\n",
    "Download tasks from facebook.ai/babi \"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "# from data_utils import load_task, vectorize_data - using data_utils code from cell above\n",
    "\n",
    "#from sklearn import cross_validation, metrics #old code\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "#metrics\n",
    "#from memn2n import MemN2N -- using MemN2N code from cell above\n",
    "\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QHAWaYYXudx"
   },
   "outputs": [],
   "source": [
    "####Delete all flags before declare#####\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "# define tasks\n",
    "# NOTICE: need to change data_dir\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate for Adam Optimizer.\")\n",
    "tf.flags.DEFINE_float(\"anneal_rate\", 15, \"Number of epochs between halving the learnign rate.\")\n",
    "tf.flags.DEFINE_float(\"anneal_stop_epoch\", 60, \"Epoch number to end annealed lr schedule.\")\n",
    "tf.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\n",
    "tf.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 60, \"Number of epochs to train for.\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 40, \"Embedding size for embedding matrices.\")\n",
    "tf.flags.DEFINE_integer(\"memory_size\", 50, \"Maximum size of memory.\")\n",
    "tf.flags.DEFINE_integer(\"random_state\", 42, \"Random state.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"/content/en\", \"Directory containing bAbI tasks\")\n",
    "tf.flags.DEFINE_string(\"output_file\", \"scores.csv\", \"Name of output file for final bAbI accuracy scores.\")\n",
    "FLAGS = tf.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6240,
     "status": "ok",
     "timestamp": 1574063439232,
     "user": {
      "displayName": "jinglin zhao",
      "photoUrl": "",
      "userId": "00443336287763192682"
     },
     "user_tz": 360
    },
    "id": "gLGuEAIyY1kp",
    "outputId": "d46e2b48-a838-481f-8bfd-5168b72e4963"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# load all train/test data\n",
    "ids = range(1, 21)\n",
    "train, test = [], []\n",
    "for i in ids:\n",
    "    tr, te = load_task(FLAGS.data_dir, i)\n",
    "    train.append(tr)\n",
    "    test.append(te)\n",
    "data = list(chain.from_iterable(train + test))\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n",
    "sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "query_size = max(map(len, (q for _, q, _ in data)))\n",
    "memory_size = min(FLAGS.memory_size, max_story_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6231,
     "status": "ok",
     "timestamp": 1574063439233,
     "user": {
      "displayName": "jinglin zhao",
      "photoUrl": "",
      "userId": "00443336287763192682"
     },
     "user_tz": 360
    },
    "id": "4JBtrtzaY8oZ",
    "outputId": "b513083e-0461-44b9-a730-8f25d1283223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence length 12\n",
      "Longest story length 228\n",
      "Average story length 9\n"
     ]
    }
   ],
   "source": [
    "# Add time words/indexes\n",
    "for i in range(memory_size):\n",
    "    word_idx['time{}'.format(i+1)] = 'time{}'.format(i+1)\n",
    "\n",
    "vocab_size = len(word_idx) + 1 # +1 for nil word\n",
    "sentence_size = max(query_size, sentence_size) # for the position\n",
    "sentence_size += 1  # +1 for time words\n",
    "\n",
    "print(\"Longest sentence length\", sentence_size)\n",
    "print(\"Longest story length\", max_story_size)\n",
    "print(\"Average story length\", mean_story_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13557,
     "status": "ok",
     "timestamp": 1574063446567,
     "user": {
      "displayName": "jinglin zhao",
      "photoUrl": "",
      "userId": "00443336287763192682"
     },
     "user_tz": 360
    },
    "id": "aXPXZYz-ZM6B",
    "outputId": "abfa2bae-a3fc-426e-ca77-789c68c43a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size 18000\n",
      "Validation Size 2000\n",
      "Testing Size 20000\n",
      "(18000, 50, 12) (2000, 50, 12) (20000, 50, 12)\n",
      "(18000, 12) (2000, 12) (20000, 12)\n",
      "(18000, 225) (2000, 225) (20000, 225)\n"
     ]
    }
   ],
   "source": [
    "# train/validation/test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "trainS = []\n",
    "valS = []\n",
    "trainQ = []\n",
    "valQ = []\n",
    "trainA = []\n",
    "valA = []\n",
    "for task in train:\n",
    "    S, Q, A = vectorize_data(task, word_idx, sentence_size, memory_size)\n",
    "    ts, vs, tq, vq, ta, va = train_test_split(S, Q, A, test_size=0.1, random_state=FLAGS.random_state)\n",
    "    trainS.append(ts)\n",
    "    trainQ.append(tq)\n",
    "    trainA.append(ta)\n",
    "    valS.append(vs)\n",
    "    valQ.append(vq)\n",
    "    valA.append(va)\n",
    "\n",
    "trainS = reduce(lambda a,b : np.vstack((a,b)), (x for x in trainS))\n",
    "trainQ = reduce(lambda a,b : np.vstack((a,b)), (x for x in trainQ))\n",
    "trainA = reduce(lambda a,b : np.vstack((a,b)), (x for x in trainA))\n",
    "valS = reduce(lambda a,b : np.vstack((a,b)), (x for x in valS))\n",
    "valQ = reduce(lambda a,b : np.vstack((a,b)), (x for x in valQ))\n",
    "valA = reduce(lambda a,b : np.vstack((a,b)), (x for x in valA))\n",
    "\n",
    "testS, testQ, testA = vectorize_data(list(chain.from_iterable(test)), word_idx, sentence_size, memory_size)\n",
    "\n",
    "n_train = trainS.shape[0]\n",
    "n_val = valS.shape[0]\n",
    "n_test = testS.shape[0]\n",
    "\n",
    "print(\"Training Size\", n_train)\n",
    "print(\"Validation Size\", n_val)\n",
    "print(\"Testing Size\", n_test)\n",
    "\n",
    "print(trainS.shape, valS.shape, testS.shape)\n",
    "print(trainQ.shape, valQ.shape, testQ.shape)\n",
    "print(trainA.shape, valA.shape, testA.shape)\n",
    "\n",
    "train_labels = np.argmax(trainA, axis=1)\n",
    "test_labels = np.argmax(testA, axis=1)\n",
    "val_labels = np.argmax(valA, axis=1)\n",
    "\n",
    "tf.set_random_seed(FLAGS.random_state)\n",
    "batch_size = FLAGS.batch_size\n",
    "\n",
    "# This avoids feeding 1 task after another, instead each batch has a random sampling of tasks\n",
    "batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "batches = [(start, end) for start,end in batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWeT9jhxZ7mr"
   },
   "source": [
    "## 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1096718,
     "status": "ok",
     "timestamp": 1574064529755,
     "user": {
      "displayName": "jinglin zhao",
      "photoUrl": "",
      "userId": "00443336287763192682"
     },
     "user_tz": 360
    },
    "id": "JVExBhljZZFC",
    "outputId": "2efea515-2c84-412e-fcc9-953e1c8f38f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-876c1e180296>:103: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "-----------------------\n",
      "Epoch 10\n",
      "Total Cost: 13019.737352088094\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 0.9888888888888889\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.974\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.7655555555555555\n",
      "Validation Accuracy = 0.71\n",
      "Testing Accuracy = 0.699\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.5288888888888889\n",
      "Validation Accuracy = 0.55\n",
      "Testing Accuracy = 0.441\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.7066666666666667\n",
      "Validation Accuracy = 0.74\n",
      "Testing Accuracy = 0.652\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.81\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.787\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.6322222222222222\n",
      "Validation Accuracy = 0.67\n",
      "Testing Accuracy = 0.632\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.8566666666666667\n",
      "Validation Accuracy = 0.88\n",
      "Testing Accuracy = 0.819\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.8833333333333333\n",
      "Validation Accuracy = 0.87\n",
      "Testing Accuracy = 0.85\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.7044444444444444\n",
      "Validation Accuracy = 0.65\n",
      "Testing Accuracy = 0.699\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.5666666666666667\n",
      "Validation Accuracy = 0.53\n",
      "Testing Accuracy = 0.579\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.8933333333333333\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.906\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9444444444444444\n",
      "Validation Accuracy = 0.93\n",
      "Testing Accuracy = 0.951\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.93\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.925\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9077777777777778\n",
      "Validation Accuracy = 0.79\n",
      "Testing Accuracy = 0.802\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 0.6133333333333333\n",
      "Validation Accuracy = 0.6\n",
      "Testing Accuracy = 0.525\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.5066666666666667\n",
      "Validation Accuracy = 0.46\n",
      "Testing Accuracy = 0.48\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.5433333333333333\n",
      "Validation Accuracy = 0.49\n",
      "Testing Accuracy = 0.498\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.4988888888888889\n",
      "Validation Accuracy = 0.46\n",
      "Testing Accuracy = 0.531\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.12666666666666668\n",
      "Validation Accuracy = 0.11\n",
      "Testing Accuracy = 0.091\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 0.9977777777777778\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.997\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 20\n",
      "Total Cost: 7244.9904863182455\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.993\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9477777777777778\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.87\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.8733333333333333\n",
      "Validation Accuracy = 0.63\n",
      "Testing Accuracy = 0.679\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8144444444444444\n",
      "Validation Accuracy = 0.81\n",
      "Testing Accuracy = 0.722\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.9633333333333334\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.841\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.97\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.926\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.8966666666666666\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.847\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.9566666666666667\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.887\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9755555555555555\n",
      "Validation Accuracy = 0.93\n",
      "Testing Accuracy = 0.937\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.9077777777777778\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.839\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9133333333333333\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.881\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9733333333333334\n",
      "Validation Accuracy = 0.95\n",
      "Testing Accuracy = 0.962\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.94\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.933\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9811111111111112\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.861\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 1.0\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.5855555555555556\n",
      "Validation Accuracy = 0.48\n",
      "Testing Accuracy = 0.425\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.5955555555555555\n",
      "Validation Accuracy = 0.51\n",
      "Testing Accuracy = 0.572\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.6677777777777778\n",
      "Validation Accuracy = 0.6\n",
      "Testing Accuracy = 0.639\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.21333333333333335\n",
      "Validation Accuracy = 0.08\n",
      "Testing Accuracy = 0.114\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 1.0\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 30\n",
      "Total Cost: 6529.232357244007\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.988\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9577777777777777\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.87\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.8788888888888889\n",
      "Validation Accuracy = 0.71\n",
      "Testing Accuracy = 0.677\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8088888888888889\n",
      "Validation Accuracy = 0.8\n",
      "Testing Accuracy = 0.736\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.9755555555555555\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.86\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.9866666666666667\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.946\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.9288888888888889\n",
      "Validation Accuracy = 0.87\n",
      "Testing Accuracy = 0.852\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.97\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.889\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9655555555555555\n",
      "Validation Accuracy = 0.93\n",
      "Testing Accuracy = 0.934\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.9522222222222222\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.865\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9177777777777778\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.877\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9788888888888889\n",
      "Validation Accuracy = 0.94\n",
      "Testing Accuracy = 0.967\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.94\n",
      "Validation Accuracy = 0.88\n",
      "Testing Accuracy = 0.919\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9766666666666667\n",
      "Validation Accuracy = 0.84\n",
      "Testing Accuracy = 0.859\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.997\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.62\n",
      "Validation Accuracy = 0.44\n",
      "Testing Accuracy = 0.474\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.6322222222222222\n",
      "Validation Accuracy = 0.49\n",
      "Testing Accuracy = 0.537\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.9144444444444444\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.872\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.25333333333333335\n",
      "Validation Accuracy = 0.13\n",
      "Testing Accuracy = 0.132\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 1.0\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 40\n",
      "Total Cost: 5048.158570178784\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.99\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9877777777777778\n",
      "Validation Accuracy = 0.94\n",
      "Testing Accuracy = 0.894\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.9166666666666666\n",
      "Validation Accuracy = 0.73\n",
      "Testing Accuracy = 0.685\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.87\n",
      "Validation Accuracy = 0.77\n",
      "Testing Accuracy = 0.756\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.9866666666666667\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.863\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.9944444444444445\n",
      "Validation Accuracy = 0.97\n",
      "Testing Accuracy = 0.954\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.9688888888888889\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.848\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.9855555555555555\n",
      "Validation Accuracy = 0.95\n",
      "Testing Accuracy = 0.888\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9944444444444445\n",
      "Validation Accuracy = 0.97\n",
      "Testing Accuracy = 0.963\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.9644444444444444\n",
      "Validation Accuracy = 0.87\n",
      "Testing Accuracy = 0.871\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9388888888888889\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.876\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9944444444444445\n",
      "Validation Accuracy = 0.94\n",
      "Testing Accuracy = 0.968\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.94\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.932\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9955555555555555\n",
      "Validation Accuracy = 0.85\n",
      "Testing Accuracy = 0.864\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.997\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.6711111111111111\n",
      "Validation Accuracy = 0.43\n",
      "Testing Accuracy = 0.469\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.6533333333333333\n",
      "Validation Accuracy = 0.58\n",
      "Testing Accuracy = 0.558\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.9444444444444444\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.914\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.2688888888888889\n",
      "Validation Accuracy = 0.09\n",
      "Testing Accuracy = 0.114\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 1.0\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 50\n",
      "Total Cost: 4425.584686419927\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.985\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9955555555555555\n",
      "Validation Accuracy = 0.93\n",
      "Testing Accuracy = 0.89\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.9344444444444444\n",
      "Validation Accuracy = 0.72\n",
      "Testing Accuracy = 0.691\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8711111111111111\n",
      "Validation Accuracy = 0.82\n",
      "Testing Accuracy = 0.772\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.9944444444444445\n",
      "Validation Accuracy = 0.85\n",
      "Testing Accuracy = 0.861\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 0.95\n",
      "Testing Accuracy = 0.952\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.9744444444444444\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.847\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.99\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.882\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.961\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.97\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.873\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9488888888888889\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.874\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 0.94\n",
      "Testing Accuracy = 0.969\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.9477777777777778\n",
      "Validation Accuracy = 0.88\n",
      "Testing Accuracy = 0.917\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 0.85\n",
      "Testing Accuracy = 0.858\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.998\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.6855555555555556\n",
      "Validation Accuracy = 0.45\n",
      "Testing Accuracy = 0.429\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.7322222222222222\n",
      "Validation Accuracy = 0.61\n",
      "Testing Accuracy = 0.567\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.9333333333333333\n",
      "Validation Accuracy = 0.87\n",
      "Testing Accuracy = 0.893\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.28888888888888886\n",
      "Validation Accuracy = 0.08\n",
      "Testing Accuracy = 0.118\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 1.0\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 60\n",
      "Total Cost: 4282.807994226925\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.983\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.878\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.9488888888888889\n",
      "Validation Accuracy = 0.72\n",
      "Testing Accuracy = 0.687\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8933333333333333\n",
      "Validation Accuracy = 0.85\n",
      "Testing Accuracy = 0.789\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.9955555555555555\n",
      "Validation Accuracy = 0.84\n",
      "Testing Accuracy = 0.865\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.95\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.9744444444444444\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.847\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.9888888888888889\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.88\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 0.97\n",
      "Testing Accuracy = 0.96\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.9766666666666667\n",
      "Validation Accuracy = 0.88\n",
      "Testing Accuracy = 0.877\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9444444444444444\n",
      "Validation Accuracy = 0.9\n",
      "Testing Accuracy = 0.865\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.956\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.9511111111111111\n",
      "Validation Accuracy = 0.88\n",
      "Testing Accuracy = 0.924\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9955555555555555\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.858\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.995\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.6788888888888889\n",
      "Validation Accuracy = 0.43\n",
      "Testing Accuracy = 0.431\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.7488888888888889\n",
      "Validation Accuracy = 0.63\n",
      "Testing Accuracy = 0.579\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.9611111111111111\n",
      "Validation Accuracy = 0.89\n",
      "Testing Accuracy = 0.909\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.3088888888888889\n",
      "Validation Accuracy = 0.11\n",
      "Testing Accuracy = 0.133\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.999\n",
      "\n",
      "-----------------------\n",
      "Writing final results to scores.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = MemN2N(batch_size, vocab_size, sentence_size, memory_size, FLAGS.embedding_size, session=sess,\n",
    "                   hops=FLAGS.hops, max_grad_norm=FLAGS.max_grad_norm)\n",
    "    for i in range(1, FLAGS.epochs+1):\n",
    "        # Stepped learning rate\n",
    "        if i - 1 <= FLAGS.anneal_stop_epoch:\n",
    "            anneal = 2.0 ** ((i - 1) // FLAGS.anneal_rate)\n",
    "        else:\n",
    "            anneal = 2.0 ** (FLAGS.anneal_stop_epoch // FLAGS.anneal_rate)\n",
    "        lr = FLAGS.learning_rate / anneal\n",
    "\n",
    "        np.random.shuffle(batches)\n",
    "        total_cost = 0.0\n",
    "        for start, end in batches:\n",
    "            s = trainS[start:end]\n",
    "            q = trainQ[start:end]\n",
    "            a = trainA[start:end]\n",
    "            cost_t = model.batch_fit(s, q, a, lr)\n",
    "            total_cost += cost_t\n",
    "\n",
    "        if i % FLAGS.evaluation_interval == 0:\n",
    "            train_accs = []\n",
    "            for start in range(0, int(n_train), int(n_train/20)): #when change number of tasks in use, change accordingly\n",
    "                end = start + int(n_train/20)\n",
    "                s = trainS[start:end]\n",
    "                q = trainQ[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                acc = accuracy_score(pred, train_labels[start:end])\n",
    "                train_accs.append(acc)\n",
    "\n",
    "            val_accs = []\n",
    "            for start in range(0, int(n_val), int(n_val/20)):\n",
    "                end = start + int(n_val/20)\n",
    "                s = valS[start:end]\n",
    "                q = valQ[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                acc = accuracy_score(pred, val_labels[start:end])\n",
    "                val_accs.append(acc)\n",
    "\n",
    "            test_accs = []\n",
    "            for start in range(0, int(n_test), int(n_test/20)):\n",
    "                end = start + int(n_test/20)\n",
    "                s = testS[start:end]\n",
    "                q = testQ[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                acc = accuracy_score(pred, test_labels[start:end])\n",
    "                test_accs.append(acc)\n",
    "\n",
    "            print('-----------------------')\n",
    "            print('Epoch', i)\n",
    "            print('Total Cost:', total_cost)\n",
    "            print()\n",
    "            t = 1\n",
    "            for t1, t2, t3 in zip(train_accs, val_accs, test_accs):\n",
    "                print(\"Task {}\".format(t))\n",
    "                print(\"Training Accuracy = {}\".format(t1))\n",
    "                print(\"Validation Accuracy = {}\".format(t2))\n",
    "                print(\"Testing Accuracy = {}\".format(t3))\n",
    "                print()\n",
    "                t += 1\n",
    "            print('-----------------------')\n",
    "\n",
    "        # Write final results to csv file\n",
    "        if i == FLAGS.epochs:\n",
    "            print('Writing final results to {}'.format(FLAGS.output_file))\n",
    "            df = pd.DataFrame({\n",
    "            'Training Accuracy': train_accs,\n",
    "            'Validation Accuracy': val_accs,\n",
    "            'Testing Accuracy': test_accs\n",
    "            }, index=range(1, 21))\n",
    "            df.index.name = 'Task'\n",
    "            df.to_csv(FLAGS.output_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_joint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
